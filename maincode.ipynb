{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb0b65b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 288x512 1 Two-Wheeler, 94.7ms\n",
      "Speed: 13.3ms preprocess, 94.7ms inference, 14.8ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 74.0ms\n",
      "Speed: 1.0ms preprocess, 74.0ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 68.0ms\n",
      "Speed: 1.4ms preprocess, 68.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 62.9ms\n",
      "Speed: 1.1ms preprocess, 62.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 65.5ms\n",
      "Speed: 1.2ms preprocess, 65.5ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 60.2ms\n",
      "Speed: 2.3ms preprocess, 60.2ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 81.4ms\n",
      "Speed: 1.6ms preprocess, 81.4ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 138.5ms\n",
      "Speed: 1.1ms preprocess, 138.5ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 76.3ms\n",
      "Speed: 1.6ms preprocess, 76.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 73.2ms\n",
      "Speed: 1.5ms preprocess, 73.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 68.8ms\n",
      "Speed: 0.9ms preprocess, 68.8ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 66.1ms\n",
      "Speed: 0.9ms preprocess, 66.1ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 65.8ms\n",
      "Speed: 0.9ms preprocess, 65.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 63.1ms\n",
      "Speed: 1.2ms preprocess, 63.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 63.0ms\n",
      "Speed: 1.2ms preprocess, 63.0ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 65.5ms\n",
      "Speed: 0.9ms preprocess, 65.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 67.9ms\n",
      "Speed: 1.7ms preprocess, 67.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 72.8ms\n",
      "Speed: 1.6ms preprocess, 72.8ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 71.4ms\n",
      "Speed: 1.1ms preprocess, 71.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 76.8ms\n",
      "Speed: 0.9ms preprocess, 76.8ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 66.0ms\n",
      "Speed: 1.3ms preprocess, 66.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 70.3ms\n",
      "Speed: 2.3ms preprocess, 70.3ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 73.7ms\n",
      "Speed: 1.1ms preprocess, 73.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 78.5ms\n",
      "Speed: 1.3ms preprocess, 78.5ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 72.0ms\n",
      "Speed: 1.1ms preprocess, 72.0ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 65.2ms\n",
      "Speed: 1.0ms preprocess, 65.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 67.9ms\n",
      "Speed: 1.0ms preprocess, 67.9ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 68.2ms\n",
      "Speed: 1.4ms preprocess, 68.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 65.4ms\n",
      "Speed: 2.1ms preprocess, 65.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 64.8ms\n",
      "Speed: 1.5ms preprocess, 64.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 63.4ms\n",
      "Speed: 0.9ms preprocess, 63.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 76.5ms\n",
      "Speed: 1.4ms preprocess, 76.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 66.6ms\n",
      "Speed: 1.3ms preprocess, 66.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Two-Wheeler, 82.8ms\n",
      "Speed: 1.2ms preprocess, 82.8ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 75.4ms\n",
      "Speed: 1.5ms preprocess, 75.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 94.6ms\n",
      "Speed: 1.0ms preprocess, 94.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 66.4ms\n",
      "Speed: 1.0ms preprocess, 66.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 61.4ms\n",
      "Speed: 0.9ms preprocess, 61.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 62.5ms\n",
      "Speed: 0.9ms preprocess, 62.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 77.2ms\n",
      "Speed: 1.0ms preprocess, 77.2ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 1 Two-Wheeler, 60.1ms\n",
      "Speed: 1.1ms preprocess, 60.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 1 Two-Wheeler, 59.5ms\n",
      "Speed: 1.1ms preprocess, 59.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 1 Two-Wheeler, 84.0ms\n",
      "Speed: 1.0ms preprocess, 84.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 1 Two-Wheeler, 53.1ms\n",
      "Speed: 1.1ms preprocess, 53.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 1 Two-Wheeler, 60.1ms\n",
      "Speed: 1.0ms preprocess, 60.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 1 Two-Wheeler, 53.5ms\n",
      "Speed: 1.1ms preprocess, 53.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 1 Two-Wheeler, 60.8ms\n",
      "Speed: 1.0ms preprocess, 60.8ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 1 Two-Wheeler, 66.6ms\n",
      "Speed: 1.0ms preprocess, 66.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 2 Cars, 62.8ms\n",
      "Speed: 1.1ms preprocess, 62.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 58.2ms\n",
      "Speed: 0.9ms preprocess, 58.2ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 2 Cars, 60.7ms\n",
      "Speed: 1.0ms preprocess, 60.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 67.8ms\n",
      "Speed: 1.2ms preprocess, 67.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 2 Cars, 63.5ms\n",
      "Speed: 1.0ms preprocess, 63.5ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 (no detections), 66.0ms\n",
      "Speed: 1.0ms preprocess, 66.0ms inference, 0.2ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 (no detections), 59.0ms\n",
      "Speed: 1.0ms preprocess, 59.0ms inference, 0.2ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 59.6ms\n",
      "Speed: 0.9ms preprocess, 59.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 2 Cars, 61.2ms\n",
      "Speed: 1.1ms preprocess, 61.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 59.6ms\n",
      "Speed: 1.1ms preprocess, 59.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 65.5ms\n",
      "Speed: 1.0ms preprocess, 65.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 58.9ms\n",
      "Speed: 1.0ms preprocess, 58.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 60.0ms\n",
      "Speed: 0.9ms preprocess, 60.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 57.6ms\n",
      "Speed: 1.1ms preprocess, 57.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 57.0ms\n",
      "Speed: 1.1ms preprocess, 57.0ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 54.3ms\n",
      "Speed: 0.9ms preprocess, 54.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 64.7ms\n",
      "Speed: 0.9ms preprocess, 64.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 56.4ms\n",
      "Speed: 1.0ms preprocess, 56.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 61.8ms\n",
      "Speed: 1.0ms preprocess, 61.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 63.7ms\n",
      "Speed: 1.0ms preprocess, 63.7ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 67.8ms\n",
      "Speed: 1.1ms preprocess, 67.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 62.5ms\n",
      "Speed: 1.1ms preprocess, 62.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 69.5ms\n",
      "Speed: 0.9ms preprocess, 69.5ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 69.5ms\n",
      "Speed: 1.1ms preprocess, 69.5ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 (no detections), 74.1ms\n",
      "Speed: 2.9ms preprocess, 74.1ms inference, 0.2ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 64.1ms\n",
      "Speed: 1.2ms preprocess, 64.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 66.7ms\n",
      "Speed: 1.0ms preprocess, 66.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 77.2ms\n",
      "Speed: 2.8ms preprocess, 77.2ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 78.8ms\n",
      "Speed: 1.0ms preprocess, 78.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 (no detections), 62.9ms\n",
      "Speed: 2.3ms preprocess, 62.9ms inference, 0.2ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 81.7ms\n",
      "Speed: 1.0ms preprocess, 81.7ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 (no detections), 71.9ms\n",
      "Speed: 0.9ms preprocess, 71.9ms inference, 0.2ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 76.5ms\n",
      "Speed: 1.5ms preprocess, 76.5ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 (no detections), 68.3ms\n",
      "Speed: 1.5ms preprocess, 68.3ms inference, 0.2ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 66.2ms\n",
      "Speed: 1.3ms preprocess, 66.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 1 Car, 70.2ms\n",
      "Speed: 1.0ms preprocess, 70.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 (no detections), 67.2ms\n",
      "Speed: 1.0ms preprocess, 67.2ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 (no detections), 71.1ms\n",
      "Speed: 1.0ms preprocess, 71.1ms inference, 0.2ms postprocess per image at shape (1, 3, 288, 512)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 153\u001b[0m\n\u001b[1;32m    150\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# Call the main function with your video path\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m process_video(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/prakhyath/Desktop/capstone project/obj detection/18th_Crs_BsStp_JN_FIX_2_time_2024-05-14T07-30-02_000.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 137\u001b[0m, in \u001b[0;36mprocess_video\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m    134\u001b[0m cropped_frame \u001b[38;5;241m=\u001b[39m frame[y1:y2, x1:x2]\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Extract date and time from the cropped frame\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m date, time \u001b[38;5;241m=\u001b[39m extract_date_time(cropped_frame)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;129;01mand\u001b[39;00m time:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Perform vehicle detection and tracking\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     results \u001b[38;5;241m=\u001b[39m model(frame)\n",
      "Cell \u001b[0;32mIn[4], line 105\u001b[0m, in \u001b[0;36mextract_date_time\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_date_time\u001b[39m(frame):\n\u001b[0;32m--> 105\u001b[0m     results \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mreadtext(frame, detail\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;66;03m# Assuming the OCR returns date in 'YYYY:MM:DD' format and time in 'HH:MM:SS' or similar\u001b[39;00m\n\u001b[1;32m    109\u001b[0m         date_str \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/easyocr/easyocr.py:468\u001b[0m, in \u001b[0;36mReader.readtext\u001b[0;34m(self, image, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, min_size, contrast_ths, adjust_contrast, filter_ths, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, y_ths, x_ths, add_margin, threshold, bbox_min_score, bbox_min_size, max_candidates, output_format)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# get the 1st result from hor & free list as self.detect returns a list of depth 3\u001b[39;00m\n\u001b[1;32m    467\u001b[0m horizontal_list, free_list \u001b[38;5;241m=\u001b[39m horizontal_list[\u001b[38;5;241m0\u001b[39m], free_list[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 468\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecognize(img_cv_grey, horizontal_list, free_list,\\\n\u001b[1;32m    469\u001b[0m                         decoder, beamWidth, batch_size,\\\n\u001b[1;32m    470\u001b[0m                         workers, allowlist, blocklist, detail, rotation_info,\\\n\u001b[1;32m    471\u001b[0m                         paragraph, contrast_ths, adjust_contrast,\\\n\u001b[1;32m    472\u001b[0m                         filter_ths, y_ths, x_ths, \u001b[38;5;28;01mFalse\u001b[39;00m, output_format)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/easyocr/easyocr.py:384\u001b[0m, in \u001b[0;36mReader.recognize\u001b[0;34m(self, img_cv_grey, horizontal_list, free_list, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, contrast_ths, adjust_contrast, filter_ths, y_ths, x_ths, reformat, output_format)\u001b[0m\n\u001b[1;32m    382\u001b[0m     f_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    383\u001b[0m     image_list, max_width \u001b[38;5;241m=\u001b[39m get_image_list(h_list, f_list, img_cv_grey, model_height \u001b[38;5;241m=\u001b[39m imgH)\n\u001b[0;32m--> 384\u001b[0m     result0 \u001b[38;5;241m=\u001b[39m get_text(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharacter, imgH, \u001b[38;5;28mint\u001b[39m(max_width), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecognizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverter, image_list,\\\n\u001b[1;32m    385\u001b[0m                   ignore_char, decoder, beamWidth, batch_size, contrast_ths, adjust_contrast, filter_ths,\\\n\u001b[1;32m    386\u001b[0m                   workers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    387\u001b[0m     result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m result0\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m free_list:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/easyocr/recognition.py:206\u001b[0m, in \u001b[0;36mget_text\u001b[0;34m(character, imgH, imgW, recognizer, converter, image_list, ignore_char, decoder, beamWidth, batch_size, contrast_ths, adjust_contrast, filter_ths, workers, device)\u001b[0m\n\u001b[1;32m    201\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m    202\u001b[0m     test_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    203\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(workers), collate_fn\u001b[38;5;241m=\u001b[39mAlignCollate_normal, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# predict first round\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m result1 \u001b[38;5;241m=\u001b[39m recognizer_predict(recognizer, converter, test_loader,batch_max_length,\\\n\u001b[1;32m    207\u001b[0m                              ignore_idx, char_group_idx, decoder, beamWidth, device \u001b[38;5;241m=\u001b[39m device)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# predict second round\u001b[39;00m\n\u001b[1;32m    210\u001b[0m low_confident_idx \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i,item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(result1) \u001b[38;5;28;01mif\u001b[39;00m (item[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m contrast_ths)]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/easyocr/recognition.py:104\u001b[0m, in \u001b[0;36mrecognizer_predict\u001b[0;34m(model, converter, test_loader, batch_max_length, ignore_idx, char_group_idx, decoder, beamWidth, device)\u001b[0m\n\u001b[1;32m    102\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m image_tensors \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m    105\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m image_tensors\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    106\u001b[0m         image \u001b[38;5;241m=\u001b[39m image_tensors\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/easyocr/recognition.py:94\u001b[0m, in \u001b[0;36mAlignCollate.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     91\u001b[0m         resized_w \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgH \u001b[38;5;241m*\u001b[39m ratio)\n\u001b[1;32m     93\u001b[0m     resized_image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mresize((resized_w, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgH), Image\u001b[38;5;241m.\u001b[39mBICUBIC)\n\u001b[0;32m---> 94\u001b[0m     resized_images\u001b[38;5;241m.\u001b[39mappend(transform(resized_image))\n\u001b[1;32m     96\u001b[0m image_tensors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([t\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m resized_images], \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image_tensors\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/easyocr/recognition.py:39\u001b[0m, in \u001b[0;36mNormalizePAD.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m---> 39\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoTensor(img)\n\u001b[1;32m     40\u001b[0m     img\u001b[38;5;241m.\u001b[39msub_(\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mdiv_(\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     41\u001b[0m     c, h, w \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:172\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[0;32m--> 172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import easyocr\n",
    "from ultralytics import YOLO\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('/Users/prakhyath/Desktop/capstone project/best.pt')\n",
    "\n",
    "# Initialize the CSV file\n",
    "csv_file = 'output6.csv'\n",
    "columns = ['Date', 'Time', 'Vehicle Class', 'Direction']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Tracker dictionary to track vehicle positions (ID: centroid)\n",
    "trackers = {}\n",
    "vehicle_id = 0\n",
    "track_memory = 50  # How long to keep track of a vehicle's centroid (for direction detection)\n",
    "\n",
    "# ROI coordinates for date and time cropping\n",
    "roi_coords = [\n",
    "    [1260, 3],\n",
    "    [1901, 12],\n",
    "    [1894, 72],\n",
    "    [1257, 62]\n",
    "]\n",
    "\n",
    "# Extract bounding box coordinates from ROI\n",
    "x1, y1 = roi_coords[0][0], roi_coords[0][1]\n",
    "x2, y2 = roi_coords[2][0], roi_coords[2][1]\n",
    "\n",
    "# Instantiate EasyOCR Reader\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Function to process vehicles and avoid duplicate storage\n",
    "def process_vehicles(frame, results, date, time):\n",
    "    global df, vehicle_id\n",
    "\n",
    "    new_rows = []\n",
    "\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            class_id = int(box.cls[0])\n",
    "            vehicle_class = model.names[class_id]\n",
    "\n",
    "            x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "            centroid_x = int((x1 + x2) / 2)\n",
    "            centroid_y = int((y1 + y2) / 2)\n",
    "\n",
    "            vehicle_detected = False\n",
    "            for v_id, track_data in trackers.items():\n",
    "                prev_centroid = track_data['centroid']\n",
    "                if np.linalg.norm(np.array([centroid_x, centroid_y]) - np.array(prev_centroid)) < 50:\n",
    "                    trackers[v_id]['centroid'] = (centroid_x, centroid_y)\n",
    "                    vehicle_detected = True\n",
    "\n",
    "                    track_y = deque(trackers[v_id]['y_coords'], maxlen=track_memory)\n",
    "                    track_y.append(centroid_y)\n",
    "                    trackers[v_id]['y_coords'] = track_y\n",
    "                    direction = get_direction(track_y)\n",
    "\n",
    "                    # Ensure the vehicle hasn't already been stored in the CSV for this session\n",
    "                    if not track_data['logged']:\n",
    "                        new_row = {\n",
    "                            'Date': str(date),  # Already formatted as YYYY-MM-DD\n",
    "                            'Time': str(time),  # Already formatted as HH:MM:SS\n",
    "                            'Vehicle Class': vehicle_class,\n",
    "                            'Direction': direction\n",
    "                        }\n",
    "                        new_rows.append(new_row)\n",
    "                        trackers[v_id]['logged'] = True  # Mark as logged\n",
    "\n",
    "                    break\n",
    "\n",
    "            # If vehicle was not detected previously, assign a new ID\n",
    "            if not vehicle_detected:\n",
    "                trackers[vehicle_id] = {\n",
    "                    'centroid': (centroid_x, centroid_y),\n",
    "                    'y_coords': deque([centroid_y], maxlen=track_memory),\n",
    "                    'logged': False  # Not yet logged in CSV\n",
    "                }\n",
    "                vehicle_id += 1\n",
    "\n",
    "    if new_rows:\n",
    "        new_df = pd.DataFrame(new_rows)\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "        # Save DataFrame to CSV\n",
    "        df.to_csv(csv_file, index=False)\n",
    "\n",
    "# Function to detect direction (up or down)\n",
    "def get_direction(y_coords):\n",
    "    if len(y_coords) < 2:\n",
    "        return 'Unknown'\n",
    "    if y_coords[0] - y_coords[-1] > 0:\n",
    "        return 'up'\n",
    "    elif y_coords[-1] - y_coords[0] > 0:\n",
    "        return 'down'\n",
    "    return 'Unknown'\n",
    "\n",
    "# Function to extract date and time from a cropped video frame\n",
    "def extract_date_time(frame):\n",
    "    results = reader.readtext(frame, detail=0)\n",
    "\n",
    "    if results and len(results) >= 2:\n",
    "        # Assuming the OCR returns date in 'YYYY:MM:DD' format and time in 'HH:MM:SS' or similar\n",
    "        date_str = results[0].replace(':', '-')\n",
    "        time_str = results[1]\n",
    "\n",
    "        # Ensure proper formatting of the date (YYYY-MM-DD) and time (HH:MM:SS)\n",
    "        try:\n",
    "            date = datetime.strptime(date_str, \"%Y-%m-%d\").date()  # Format date\n",
    "            time = datetime.strptime(time_str, \"%H:%M:%S\").time()  # Format time\n",
    "        except ValueError:\n",
    "            # If the parsing fails, return None\n",
    "            return None, None\n",
    "\n",
    "        return date, time\n",
    "\n",
    "    return None, None\n",
    "\n",
    "# Main function to process the video\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Crop the frame using the provided ROI coordinates for date and time\n",
    "        cropped_frame = frame[y1:y2, x1:x2]\n",
    "\n",
    "        # Extract date and time from the cropped frame\n",
    "        date, time = extract_date_time(cropped_frame)\n",
    "\n",
    "        if date and time:\n",
    "            # Perform vehicle detection and tracking\n",
    "            results = model(frame)\n",
    "\n",
    "            # Process vehicles and save them to CSV\n",
    "            process_vehicles(frame, results, date, time)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the main function with your video path\n",
    "process_video('/Users/prakhyath/Desktop/capstone project/obj detection/18th_Crs_BsStp_JN_FIX_2_time_2024-05-14T07-30-02_000.mp4')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
